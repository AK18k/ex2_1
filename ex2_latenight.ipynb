{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AK18k/ex2_1/blob/avis_lab/ex2_latenight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b avis_lab https://github.com/AK18k/ex2_1\n",
        "\n"
      ],
      "metadata": {
        "id": "K1j_jwEwF_MG",
        "outputId": "9bee7cfa-04a1-4848-d924-362d9c9290d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ex2_1'...\n",
            "remote: Enumerating objects: 129, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/17)\u001b[K\rremote: Counting objects:  11% (2/17)\u001b[K\rremote: Counting objects:  17% (3/17)\u001b[K\rremote: Counting objects:  23% (4/17)\u001b[K\rremote: Counting objects:  29% (5/17)\u001b[K\rremote: Counting objects:  35% (6/17)\u001b[K\rremote: Counting objects:  41% (7/17)\u001b[K\rremote: Counting objects:  47% (8/17)\u001b[K\rremote: Counting objects:  52% (9/17)\u001b[K\rremote: Counting objects:  58% (10/17)\u001b[K\rremote: Counting objects:  64% (11/17)\u001b[K\rremote: Counting objects:  70% (12/17)\u001b[K\rremote: Counting objects:  76% (13/17)\u001b[K\rremote: Counting objects:  82% (14/17)\u001b[K\rremote: Counting objects:  88% (15/17)\u001b[K\rremote: Counting objects:  94% (16/17)\u001b[K\rremote: Counting objects: 100% (17/17)\u001b[K\rremote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 129 (delta 6), reused 0 (delta 0), pack-reused 112\u001b[K\n",
            "Receiving objects: 100% (129/129), 2.38 MiB | 11.64 MiB/s, done.\n",
            "Resolving deltas: 100% (60/60), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DATA_PATH = '/content/ex2_1/data/ptb'\n",
        "PATH = '/content/ex2_1'\n",
        "os.chdir('/content/ex2_1')\n",
        "!ls"
      ],
      "metadata": {
        "id": "1TkQzsA-FqiH",
        "outputId": "b2bd7f2a-555d-4a82-eba0-fd32a4774e64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "data\t ex2_1\t\t      generate.py  model.py\tREADME.md\n",
            "data.py  ex2_latenight.ipynb  main.ipynb   __pycache__\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "GW4fCoGkFZvA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'device is {device}')"
      ],
      "metadata": {
        "id": "_962w4A9wmZa",
        "outputId": "bec3ce55-8723-42f5-bcd3-3107f387173c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device is cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# first model start\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BnxIW8So_tt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Result_Matrix:\n",
        "    def __init__(self, model_type):\n",
        "      self.model_name = model_type\n",
        "      self.nepochs = 0\n",
        "      self.train_ppl = np.array([])\n",
        "      self.val_ppl = np.array([])\n",
        "      self.test_ppl = np.array([])\n",
        "\n",
        "    def add_result(self, result, result_type = 'train'):\n",
        "      if result_type == 'train':\n",
        "        self.train_ppl = np.append(self.train_ppl, result)\n",
        "      elif result_type == 'val':\n",
        "        self.val_ppl = np.append(self.val_ppl, result)\n",
        "      elif result_type == 'test':\n",
        "        self.test_ppl = np.append(self.test_ppl, result)\n",
        "\n",
        "    def get_results(self, result_type = 'train'):\n",
        "      if result_type == 'train':\n",
        "        return(self.train_ppl)\n",
        "      elif result_type == 'val':\n",
        "        return(self.val_ppl)\n",
        "      elif result_type == 'test':\n",
        "        return(self.test_ppl) \n",
        "\n",
        "\n",
        "def plot_results(result_matrix, file_name):\n",
        "  plt.figure()\n",
        "  y = result_matrix.get_results('train')\n",
        "  plt.plot(np.arange(1,len(y)+1), y, label = 'train', color = 'blue')\n",
        "  y = result_matrix.get_results('val')\n",
        "  plt.plot(np.arange(1,len(y)+1), y, label = 'val', color = 'green')\n",
        "  y = result_matrix.get_results('test')\n",
        "  plt.plot(np.arange(1,len(y)+1), y, label = 'test', color = 'red')\n",
        "  plt.legend()\n",
        "  plt.title(f'Model Type = {result_matrix.model_name}')\n",
        "  #plt.ylim((0.75, 1))\n",
        "  plt.xlabel('eoch')\n",
        "  plt.ylabel('perplexity')\n",
        "  plt.grid()\n",
        "  plt.show() \n",
        "\n",
        "  plt.savefig(file_name) \n"
      ],
      "metadata": {
        "id": "fB5XPLnpFhzI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.onnx\n",
        "\n",
        "import data\n",
        "import model\n",
        "\n",
        "parser = argparse.ArgumentParser(description='PyTorch Wikitext-2 RNN/LSTM/GRU/Transformer Language Model')\n",
        "parser.add_argument('--data', type=str, default='./data/ptb',\n",
        "                    help='location of the data corpus')\n",
        "parser.add_argument('--model', type=str, default='LSTM',\n",
        "                    help='type of network (RNN_TANH, RNN_RELU, LSTM, GRU, Transformer)')\n",
        "parser.add_argument('--emsize', type=int, default=200,\n",
        "                    help='size of word embeddings')\n",
        "parser.add_argument('--nhid', type=int, default=200,\n",
        "                    help='number of hidden units per layer')\n",
        "parser.add_argument('--nlayers', type=int, default=2,\n",
        "                    help='number of layers')\n",
        "parser.add_argument('--lr', type=float, default=20,\n",
        "                    help='initial learning rate')\n",
        "parser.add_argument('--clip', type=float, default=0.25,\n",
        "                    help='gradient clipping')\n",
        "parser.add_argument('--epochs', type=int, default=40,\n",
        "                    help='upper epoch limit')\n",
        "parser.add_argument('--batch_size', type=int, default=20, metavar='N',\n",
        "                    help='batch size')\n",
        "parser.add_argument('--bptt', type=int, default=35,\n",
        "                    help='sequence length')\n",
        "parser.add_argument('--dropout', type=float, default=0.2,\n",
        "                    help='dropout applied to layers (0 = no dropout)')\n",
        "parser.add_argument('--tied', action='store_true',\n",
        "                    help='tie the word embedding and softmax weights')\n",
        "parser.add_argument('--seed', type=int, default=1111,\n",
        "                    help='random seed')\n",
        "parser.add_argument('--cuda', action='store_true', default=True,\n",
        "                    help='use CUDA')\n",
        "parser.add_argument('--mps', action='store_true', default=False,\n",
        "                        help='enables macOS GPU training')\n",
        "parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n",
        "                    help='report interval')\n",
        "parser.add_argument('--save', type=str, default='model.pt',\n",
        "                    help='path to save the final model')\n",
        "parser.add_argument('--onnx-export', type=str, default='',\n",
        "                    help='path to export the final model in onnx format')\n",
        "parser.add_argument('--nhead', type=int, default=2,\n",
        "                    help='the number of heads in the encoder/decoder of the transformer model')\n",
        "parser.add_argument('--dry-run', action='store_true',\n",
        "                    help='verify the code and the model')\n",
        "# args = parser.parse_args()\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(args.seed)\n",
        "if torch.cuda.is_available():\n",
        "    if not args.cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda.\")\n",
        "if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    if not args.mps:\n",
        "        print(\"WARNING: You have mps device, to enable macOS GPU run with --mps.\")\n",
        "\n",
        "use_mps = args.mps and torch.backends.mps.is_available()\n",
        "if args.cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('CUDA is active')\n",
        "elif use_mps:\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "corpus = data.Corpus(args.data)\n",
        "\n",
        "# Starting from sequential data, batchify arranges the dataset into columns.\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
        "# ┌ a g m s ┐\n",
        "# │ b h n t │\n",
        "# │ c i o u │\n",
        "# │ d j p v │\n",
        "# │ e k q w │\n",
        "# └ f l r x ┘.\n",
        "# These columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
        "# batch processing.\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, args.batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "ntokens = len(corpus.dictionary)\n",
        "if args.model == 'Transformer':\n",
        "    model = model.TransformerModel(ntokens, args.emsize, args.nhead, args.nhid, args.nlayers, args.dropout).to(device)\n",
        "else:\n",
        "    model = model.RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied).to(device)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "# get_batch subdivides the source data into chunks of length args.bptt.\n",
        "# If source is equal to the example output of the batchify function, with\n",
        "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
        "# ┌ a g m s ┐ ┌ b h n t ┐\n",
        "# └ b h n t ┘ └ c i o u ┘\n",
        "# Note that despite the name of the function, the subdivison of data is not\n",
        "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
        "# by the batchify function. The chunks are along dimension 0, corresponding\n",
        "# to the seq_len dimension in the LSTM.\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if args.model != 'Transformer':\n",
        "        hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, args.bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            if args.model == 'Transformer':\n",
        "                output = model(data)\n",
        "                output = output.view(-1, ntokens)\n",
        "            else:\n",
        "                output, hidden = model(data, hidden)\n",
        "                hidden = repackage_hidden(hidden)\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if args.model != 'Transformer':\n",
        "        hidden = model.init_hidden(args.batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        model.zero_grad()\n",
        "        if args.model == 'Transformer':\n",
        "            output = model(data)\n",
        "            output = output.view(-1, ntokens)\n",
        "        else:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            output, hidden = model(data, hidden)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
        "        for p in model.parameters():\n",
        "            p.data.add_(p.grad, alpha=-lr)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % args.log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / args.log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // args.bptt, lr,\n",
        "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            \n",
        "            results_m.add_result(math.exp(cur_loss), 'train') #add train ppl to result matrix\n",
        "\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "        if args.dry_run:\n",
        "            break\n",
        "\n",
        "\n",
        "def export_onnx(path, batch_size, seq_len):\n",
        "    print('The model is also exported in ONNX format at {}.'.format(os.path.realpath(args.onnx_export)))\n",
        "    model.eval()\n",
        "    dummy_input = torch.LongTensor(seq_len * batch_size).zero_().view(-1, batch_size).to(device)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    torch.onnx.export(model, (dummy_input, hidden), path)\n",
        "\n",
        "\n",
        "# Loop over epochs.\n",
        "lr = args.lr\n",
        "best_val_loss = None\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    results_m = Result_Matrix(args.model) #create a result matrix for the model\n",
        "  \n",
        "    for epoch in range(1, args.epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        \n",
        "        results_m.add_result(math.exp(val_loss), 'val') #add val ppl to result matrix\n",
        "\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(args.save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(args.save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # after load the rnn params are not a continuous chunk of memory\n",
        "    # this makes them a continuous chunk, and will speed up forward pass\n",
        "    # Currently, only rnn model supports flatten_parameters function.\n",
        "    if args.model in ['RNN_TANH', 'RNN_RELU', 'LSTM', 'GRU']:\n",
        "        model.rnn.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)\n",
        "\n",
        "results_m.add_result(math.exp(test_loss), 'test') #add test ppl to result matrix\n",
        "\n",
        "plot_results(results_m)\n",
        "\n",
        "if len(args.onnx_export) > 0:\n",
        "    # Export the model in ONNX format.\n",
        "    export_onnx(args.onnx_export, batch_size=1, seq_len=args.bptt)"
      ],
      "metadata": {
        "id": "p0w9cFjq_oeH",
        "outputId": "d97cbc28-53f7-4d19-b820-8a3ebdee1a2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is active\n",
            "| epoch   1 |   200/ 1327 batches | lr 20.00 | ms/batch 11.27 | loss  6.92 | ppl  1016.42\n",
            "| epoch   1 |   400/ 1327 batches | lr 20.00 | ms/batch  6.11 | loss  6.30 | ppl   545.22\n",
            "| epoch   1 |   600/ 1327 batches | lr 20.00 | ms/batch  6.13 | loss  6.05 | ppl   423.20\n",
            "| epoch   1 |   800/ 1327 batches | lr 20.00 | ms/batch  6.26 | loss  5.77 | ppl   321.89\n",
            "| epoch   1 |  1000/ 1327 batches | lr 20.00 | ms/batch  6.48 | loss  5.63 | ppl   278.37\n",
            "| epoch   1 |  1200/ 1327 batches | lr 20.00 | ms/batch  6.50 | loss  5.47 | ppl   237.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time:  9.70s | valid loss  5.38 | valid ppl   217.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 1327 batches | lr 20.00 | ms/batch  6.15 | loss  5.38 | ppl   217.06\n",
            "| epoch   2 |   400/ 1327 batches | lr 20.00 | ms/batch  6.16 | loss  5.32 | ppl   204.08\n",
            "| epoch   2 |   600/ 1327 batches | lr 20.00 | ms/batch  6.17 | loss  5.27 | ppl   194.48\n",
            "| epoch   2 |   800/ 1327 batches | lr 20.00 | ms/batch  6.15 | loss  5.16 | ppl   174.10\n",
            "| epoch   2 |  1000/ 1327 batches | lr 20.00 | ms/batch  6.21 | loss  5.15 | ppl   172.90\n",
            "| epoch   2 |  1200/ 1327 batches | lr 20.00 | ms/batch  6.19 | loss  5.04 | ppl   154.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time:  8.53s | valid loss  5.08 | valid ppl   160.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 1327 batches | lr 20.00 | ms/batch  6.32 | loss  5.04 | ppl   154.86\n",
            "| epoch   3 |   400/ 1327 batches | lr 20.00 | ms/batch  6.53 | loss  5.02 | ppl   151.79\n",
            "| epoch   3 |   600/ 1327 batches | lr 20.00 | ms/batch  6.56 | loss  4.99 | ppl   147.54\n",
            "| epoch   3 |   800/ 1327 batches | lr 20.00 | ms/batch  6.26 | loss  4.91 | ppl   136.26\n",
            "| epoch   3 |  1000/ 1327 batches | lr 20.00 | ms/batch  6.23 | loss  4.95 | ppl   140.68\n",
            "| epoch   3 |  1200/ 1327 batches | lr 20.00 | ms/batch  6.27 | loss  4.83 | ppl   125.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time:  8.77s | valid loss  4.94 | valid ppl   139.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 1327 batches | lr 20.00 | ms/batch  6.28 | loss  4.86 | ppl   128.59\n",
            "| epoch   4 |   400/ 1327 batches | lr 20.00 | ms/batch  6.26 | loss  4.85 | ppl   128.31\n",
            "| epoch   4 |   600/ 1327 batches | lr 20.00 | ms/batch  6.21 | loss  4.83 | ppl   125.15\n",
            "| epoch   4 |   800/ 1327 batches | lr 20.00 | ms/batch  6.21 | loss  4.76 | ppl   117.23\n",
            "| epoch   4 |  1000/ 1327 batches | lr 20.00 | ms/batch  6.42 | loss  4.81 | ppl   123.00\n",
            "| epoch   4 |  1200/ 1327 batches | lr 20.00 | ms/batch  6.63 | loss  4.70 | ppl   109.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time:  8.87s | valid loss  4.86 | valid ppl   128.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 1327 batches | lr 20.00 | ms/batch  6.25 | loss  4.73 | ppl   113.39\n",
            "| epoch   5 |   400/ 1327 batches | lr 20.00 | ms/batch  6.21 | loss  4.73 | ppl   113.63\n",
            "| epoch   5 |   600/ 1327 batches | lr 20.00 | ms/batch  6.22 | loss  4.71 | ppl   111.59\n",
            "| epoch   5 |   800/ 1327 batches | lr 20.00 | ms/batch  6.27 | loss  4.65 | ppl   104.89\n",
            "| epoch   5 |  1000/ 1327 batches | lr 20.00 | ms/batch  6.20 | loss  4.71 | ppl   111.30\n",
            "| epoch   5 |  1200/ 1327 batches | lr 20.00 | ms/batch  6.23 | loss  4.59 | ppl    98.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time:  8.60s | valid loss  4.80 | valid ppl   122.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 1327 batches | lr 20.00 | ms/batch  6.27 | loss  4.64 | ppl   103.88\n",
            "| epoch   6 |   400/ 1327 batches | lr 20.00 | ms/batch  6.56 | loss  4.64 | ppl   103.59\n",
            "| epoch   6 |   600/ 1327 batches | lr 20.00 | ms/batch  6.55 | loss  4.63 | ppl   102.51\n",
            "| epoch   6 |   800/ 1327 batches | lr 20.00 | ms/batch  6.41 | loss  4.57 | ppl    96.63\n",
            "| epoch   6 |  1000/ 1327 batches | lr 20.00 | ms/batch  6.28 | loss  4.64 | ppl   103.54\n",
            "| epoch   6 |  1200/ 1327 batches | lr 20.00 | ms/batch  6.24 | loss  4.52 | ppl    91.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time:  8.78s | valid loss  4.77 | valid ppl   118.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/ 1327 batches | lr 20.00 | ms/batch  6.29 | loss  4.57 | ppl    96.81\n",
            "| epoch   7 |   400/ 1327 batches | lr 20.00 | ms/batch  6.31 | loss  4.58 | ppl    97.12\n",
            "| epoch   7 |   600/ 1327 batches | lr 20.00 | ms/batch  6.28 | loss  4.56 | ppl    95.94\n",
            "| epoch   7 |   800/ 1327 batches | lr 20.00 | ms/batch  7.46 | loss  4.51 | ppl    90.62\n",
            "| epoch   7 |  1000/ 1327 batches | lr 20.00 | ms/batch 10.86 | loss  4.58 | ppl    97.25\n",
            "| epoch   7 |  1200/ 1327 batches | lr 20.00 | ms/batch  6.56 | loss  4.46 | ppl    86.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time:  9.92s | valid loss  4.75 | valid ppl   115.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/ 1327 batches | lr 20.00 | ms/batch  6.30 | loss  4.52 | ppl    91.51\n",
            "| epoch   8 |   400/ 1327 batches | lr 20.00 | ms/batch  6.29 | loss  4.52 | ppl    92.06\n",
            "| epoch   8 |   600/ 1327 batches | lr 20.00 | ms/batch  6.33 | loss  4.51 | ppl    90.74\n",
            "| epoch   8 |   800/ 1327 batches | lr 20.00 | ms/batch  6.31 | loss  4.46 | ppl    86.40\n",
            "| epoch   8 |  1000/ 1327 batches | lr 20.00 | ms/batch  6.34 | loss  4.53 | ppl    92.76\n",
            "| epoch   8 |  1200/ 1327 batches | lr 20.00 | ms/batch  6.38 | loss  4.41 | ppl    82.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time:  9.00s | valid loss  4.73 | valid ppl   112.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/ 1327 batches | lr 20.00 | ms/batch  6.60 | loss  4.47 | ppl    87.57\n",
            "| epoch   9 |   400/ 1327 batches | lr 20.00 | ms/batch  6.61 | loss  4.48 | ppl    88.22\n",
            "| epoch   9 |   600/ 1327 batches | lr 20.00 | ms/batch  6.58 | loss  4.47 | ppl    87.02\n",
            "| epoch   9 |   800/ 1327 batches | lr 20.00 | ms/batch  6.39 | loss  4.42 | ppl    82.70\n",
            "| epoch   9 |  1000/ 1327 batches | lr 20.00 | ms/batch  6.38 | loss  4.49 | ppl    89.43\n",
            "| epoch   9 |  1200/ 1327 batches | lr 20.00 | ms/batch  6.44 | loss  4.37 | ppl    78.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time:  9.18s | valid loss  4.72 | valid ppl   112.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/ 1327 batches | lr 20.00 | ms/batch  7.40 | loss  4.43 | ppl    83.99\n",
            "| epoch  10 |   400/ 1327 batches | lr 20.00 | ms/batch  6.43 | loss  4.44 | ppl    84.89\n",
            "| epoch  10 |   600/ 1327 batches | lr 20.00 | ms/batch  6.51 | loss  4.43 | ppl    83.85\n",
            "| epoch  10 |   800/ 1327 batches | lr 20.00 | ms/batch  6.74 | loss  4.38 | ppl    79.85\n",
            "| epoch  10 |  1000/ 1327 batches | lr 20.00 | ms/batch  6.75 | loss  4.45 | ppl    86.02\n",
            "| epoch  10 |  1200/ 1327 batches | lr 20.00 | ms/batch  6.68 | loss  4.34 | ppl    76.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time:  9.27s | valid loss  4.71 | valid ppl   111.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   200/ 1327 batches | lr 20.00 | ms/batch  6.51 | loss  4.40 | ppl    81.28\n",
            "| epoch  11 |   400/ 1327 batches | lr 20.00 | ms/batch  6.61 | loss  4.41 | ppl    81.98\n",
            "| epoch  11 |   600/ 1327 batches | lr 20.00 | ms/batch  6.50 | loss  4.40 | ppl    81.16\n",
            "| epoch  11 |   800/ 1327 batches | lr 20.00 | ms/batch  6.57 | loss  4.35 | ppl    77.56\n",
            "| epoch  11 |  1000/ 1327 batches | lr 20.00 | ms/batch  6.52 | loss  4.42 | ppl    83.25\n",
            "| epoch  11 |  1200/ 1327 batches | lr 20.00 | ms/batch  6.59 | loss  4.31 | ppl    74.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time:  9.15s | valid loss  4.71 | valid ppl   110.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   200/ 1327 batches | lr 20.00 | ms/batch  6.82 | loss  4.37 | ppl    79.36\n",
            "| epoch  12 |   400/ 1327 batches | lr 20.00 | ms/batch  6.74 | loss  4.38 | ppl    79.97\n",
            "| epoch  12 |   600/ 1327 batches | lr 20.00 | ms/batch  6.64 | loss  4.37 | ppl    79.22\n",
            "| epoch  12 |   800/ 1327 batches | lr 20.00 | ms/batch  6.58 | loss  4.32 | ppl    75.46\n",
            "| epoch  12 |  1000/ 1327 batches | lr 20.00 | ms/batch  6.63 | loss  4.40 | ppl    81.73\n",
            "| epoch  12 |  1200/ 1327 batches | lr 20.00 | ms/batch  6.63 | loss  4.28 | ppl    72.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time:  9.18s | valid loss  4.70 | valid ppl   110.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   200/ 1327 batches | lr 20.00 | ms/batch  6.70 | loss  4.35 | ppl    77.33\n",
            "| epoch  13 |   400/ 1327 batches | lr 20.00 | ms/batch  6.68 | loss  4.36 | ppl    78.18\n",
            "| epoch  13 |   600/ 1327 batches | lr 20.00 | ms/batch  6.79 | loss  4.35 | ppl    77.77\n",
            "| epoch  13 |   800/ 1327 batches | lr 20.00 | ms/batch  6.88 | loss  4.30 | ppl    73.65\n",
            "| epoch  13 |  1000/ 1327 batches | lr 20.00 | ms/batch  6.80 | loss  4.38 | ppl    80.17\n",
            "| epoch  13 |  1200/ 1327 batches | lr 20.00 | ms/batch  6.73 | loss  4.26 | ppl    70.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time:  9.32s | valid loss  4.71 | valid ppl   110.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   200/ 1327 batches | lr 5.00 | ms/batch  6.73 | loss  4.30 | ppl    74.06\n",
            "| epoch  14 |   400/ 1327 batches | lr 5.00 | ms/batch  6.74 | loss  4.27 | ppl    71.21\n",
            "| epoch  14 |   600/ 1327 batches | lr 5.00 | ms/batch  6.75 | loss  4.23 | ppl    68.60\n",
            "| epoch  14 |   800/ 1327 batches | lr 5.00 | ms/batch  6.74 | loss  4.15 | ppl    63.18\n",
            "| epoch  14 |  1000/ 1327 batches | lr 5.00 | ms/batch  6.75 | loss  4.20 | ppl    66.45\n",
            "| epoch  14 |  1200/ 1327 batches | lr 5.00 | ms/batch  6.88 | loss  4.04 | ppl    56.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time:  9.43s | valid loss  4.62 | valid ppl   101.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   200/ 1327 batches | lr 5.00 | ms/batch  6.87 | loss  4.20 | ppl    66.56\n",
            "| epoch  15 |   400/ 1327 batches | lr 5.00 | ms/batch  6.76 | loss  4.18 | ppl    65.49\n",
            "| epoch  15 |   600/ 1327 batches | lr 5.00 | ms/batch  6.75 | loss  4.16 | ppl    64.21\n",
            "| epoch  15 |   800/ 1327 batches | lr 5.00 | ms/batch  6.75 | loss  4.09 | ppl    59.87\n",
            "| epoch  15 |  1000/ 1327 batches | lr 5.00 | ms/batch  6.72 | loss  4.16 | ppl    63.97\n",
            "| epoch  15 |  1200/ 1327 batches | lr 5.00 | ms/batch  6.75 | loss  4.01 | ppl    55.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time:  9.31s | valid loss  4.61 | valid ppl   100.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   200/ 1327 batches | lr 5.00 | ms/batch  6.67 | loss  4.16 | ppl    63.88\n",
            "| epoch  16 |   400/ 1327 batches | lr 5.00 | ms/batch  6.86 | loss  4.14 | ppl    63.10\n",
            "| epoch  16 |   600/ 1327 batches | lr 5.00 | ms/batch  6.84 | loss  4.13 | ppl    62.13\n",
            "| epoch  16 |   800/ 1327 batches | lr 5.00 | ms/batch  6.79 | loss  4.06 | ppl    58.13\n",
            "| epoch  16 |  1000/ 1327 batches | lr 5.00 | ms/batch  6.71 | loss  4.13 | ppl    62.10\n",
            "| epoch  16 |  1200/ 1327 batches | lr 5.00 | ms/batch  6.68 | loss  3.99 | ppl    54.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time:  9.30s | valid loss  4.60 | valid ppl    99.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   200/ 1327 batches | lr 5.00 | ms/batch  6.65 | loss  4.13 | ppl    61.92\n",
            "| epoch  17 |   400/ 1327 batches | lr 5.00 | ms/batch  6.72 | loss  4.12 | ppl    61.61\n",
            "| epoch  17 |   600/ 1327 batches | lr 5.00 | ms/batch  6.70 | loss  4.10 | ppl    60.45\n",
            "| epoch  17 |   800/ 1327 batches | lr 5.00 | ms/batch  6.67 | loss  4.04 | ppl    56.80\n",
            "| epoch  17 |  1000/ 1327 batches | lr 5.00 | ms/batch  6.90 | loss  4.11 | ppl    61.14\n",
            "| epoch  17 |  1200/ 1327 batches | lr 5.00 | ms/batch  6.89 | loss  3.98 | ppl    53.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time:  9.32s | valid loss  4.60 | valid ppl    99.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   200/ 1327 batches | lr 5.00 | ms/batch  6.58 | loss  4.10 | ppl    60.36\n",
            "| epoch  18 |   400/ 1327 batches | lr 5.00 | ms/batch  6.63 | loss  4.10 | ppl    60.22\n",
            "| epoch  18 |   600/ 1327 batches | lr 5.00 | ms/batch  6.64 | loss  4.08 | ppl    59.31\n",
            "| epoch  18 |   800/ 1327 batches | lr 5.00 | ms/batch  6.57 | loss  4.02 | ppl    55.70\n",
            "| epoch  18 |  1000/ 1327 batches | lr 5.00 | ms/batch  6.65 | loss  4.10 | ppl    60.29\n",
            "| epoch  18 |  1200/ 1327 batches | lr 5.00 | ms/batch  6.57 | loss  3.96 | ppl    52.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time:  9.12s | valid loss  4.60 | valid ppl    99.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   200/ 1327 batches | lr 5.00 | ms/batch  6.75 | loss  4.08 | ppl    59.27\n",
            "| epoch  19 |   400/ 1327 batches | lr 5.00 | ms/batch  6.89 | loss  4.08 | ppl    58.86\n",
            "| epoch  19 |   600/ 1327 batches | lr 5.00 | ms/batch  6.77 | loss  4.07 | ppl    58.50\n",
            "| epoch  19 |   800/ 1327 batches | lr 5.00 | ms/batch  6.74 | loss  4.01 | ppl    54.93\n",
            "| epoch  19 |  1000/ 1327 batches | lr 5.00 | ms/batch  6.62 | loss  4.08 | ppl    59.31\n",
            "| epoch  19 |  1200/ 1327 batches | lr 5.00 | ms/batch  6.59 | loss  3.95 | ppl    52.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time:  9.26s | valid loss  4.60 | valid ppl    99.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   200/ 1327 batches | lr 5.00 | ms/batch  6.68 | loss  4.06 | ppl    58.17\n",
            "| epoch  20 |   400/ 1327 batches | lr 5.00 | ms/batch  6.61 | loss  4.06 | ppl    58.03\n",
            "| epoch  20 |   600/ 1327 batches | lr 5.00 | ms/batch  6.68 | loss  4.05 | ppl    57.49\n",
            "| epoch  20 |   800/ 1327 batches | lr 5.00 | ms/batch  6.76 | loss  3.99 | ppl    54.26\n",
            "| epoch  20 |  1000/ 1327 batches | lr 5.00 | ms/batch  6.83 | loss  4.07 | ppl    58.56\n",
            "| epoch  20 |  1200/ 1327 batches | lr 5.00 | ms/batch  6.75 | loss  3.94 | ppl    51.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time:  9.25s | valid loss  4.60 | valid ppl    99.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   200/ 1327 batches | lr 5.00 | ms/batch  6.71 | loss  4.05 | ppl    57.42\n",
            "| epoch  21 |   400/ 1327 batches | lr 5.00 | ms/batch  6.58 | loss  4.04 | ppl    56.91\n",
            "| epoch  21 |   600/ 1327 batches | lr 5.00 | ms/batch  6.68 | loss  4.03 | ppl    56.42\n",
            "| epoch  21 |   800/ 1327 batches | lr 5.00 | ms/batch  6.66 | loss  3.98 | ppl    53.59\n",
            "| epoch  21 |  1000/ 1327 batches | lr 5.00 | ms/batch  6.63 | loss  4.06 | ppl    57.73\n",
            "| epoch  21 |  1200/ 1327 batches | lr 5.00 | ms/batch  6.66 | loss  3.92 | ppl    50.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time:  9.27s | valid loss  4.60 | valid ppl    99.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   200/ 1327 batches | lr 5.00 | ms/batch  6.86 | loss  4.04 | ppl    56.80\n",
            "| epoch  22 |   400/ 1327 batches | lr 5.00 | ms/batch  6.76 | loss  4.03 | ppl    56.51\n",
            "| epoch  22 |   600/ 1327 batches | lr 5.00 | ms/batch  6.68 | loss  4.03 | ppl    56.01\n",
            "| epoch  22 |   800/ 1327 batches | lr 5.00 | ms/batch  6.67 | loss  3.97 | ppl    53.04\n",
            "| epoch  22 |  1000/ 1327 batches | lr 5.00 | ms/batch  6.67 | loss  4.05 | ppl    57.16\n",
            "| epoch  22 |  1200/ 1327 batches | lr 5.00 | ms/batch  6.71 | loss  3.92 | ppl    50.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time:  9.28s | valid loss  4.60 | valid ppl    99.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   200/ 1327 batches | lr 1.25 | ms/batch  6.74 | loss  4.03 | ppl    56.35\n",
            "| epoch  23 |   400/ 1327 batches | lr 1.25 | ms/batch  6.69 | loss  4.02 | ppl    55.62\n",
            "| epoch  23 |   600/ 1327 batches | lr 1.25 | ms/batch  6.81 | loss  4.00 | ppl    54.54\n",
            "| epoch  23 |   800/ 1327 batches | lr 1.25 | ms/batch  6.88 | loss  3.93 | ppl    50.86\n",
            "| epoch  23 |  1000/ 1327 batches | lr 1.25 | ms/batch  6.77 | loss  3.99 | ppl    54.26\n",
            "| epoch  23 |  1200/ 1327 batches | lr 1.25 | ms/batch  6.72 | loss  3.85 | ppl    47.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time:  9.32s | valid loss  4.59 | valid ppl    98.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |   200/ 1327 batches | lr 1.25 | ms/batch  6.70 | loss  4.01 | ppl    55.12\n",
            "| epoch  24 |   400/ 1327 batches | lr 1.25 | ms/batch  6.69 | loss  4.00 | ppl    54.47\n",
            "| epoch  24 |   600/ 1327 batches | lr 1.25 | ms/batch  6.66 | loss  3.98 | ppl    53.73\n",
            "| epoch  24 |   800/ 1327 batches | lr 1.25 | ms/batch  6.68 | loss  3.91 | ppl    50.11\n",
            "| epoch  24 |  1000/ 1327 batches | lr 1.25 | ms/batch  6.68 | loss  3.99 | ppl    53.87\n",
            "| epoch  24 |  1200/ 1327 batches | lr 1.25 | ms/batch  6.79 | loss  3.85 | ppl    46.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time:  9.34s | valid loss  4.58 | valid ppl    97.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   200/ 1327 batches | lr 1.25 | ms/batch  6.88 | loss  3.99 | ppl    54.21\n",
            "| epoch  25 |   400/ 1327 batches | lr 1.25 | ms/batch  6.66 | loss  3.99 | ppl    53.81\n",
            "| epoch  25 |   600/ 1327 batches | lr 1.25 | ms/batch  6.72 | loss  3.97 | ppl    53.05\n",
            "| epoch  25 |   800/ 1327 batches | lr 1.25 | ms/batch  6.67 | loss  3.91 | ppl    49.70\n",
            "| epoch  25 |  1000/ 1327 batches | lr 1.25 | ms/batch  6.63 | loss  3.98 | ppl    53.40\n",
            "| epoch  25 |  1200/ 1327 batches | lr 1.25 | ms/batch  6.69 | loss  3.84 | ppl    46.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time:  9.24s | valid loss  4.58 | valid ppl    97.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   200/ 1327 batches | lr 1.25 | ms/batch  6.73 | loss  3.99 | ppl    53.86\n",
            "| epoch  26 |   400/ 1327 batches | lr 1.25 | ms/batch  6.84 | loss  3.98 | ppl    53.58\n",
            "| epoch  26 |   600/ 1327 batches | lr 1.25 | ms/batch  6.90 | loss  3.97 | ppl    52.76\n",
            "| epoch  26 |   800/ 1327 batches | lr 1.25 | ms/batch  6.80 | loss  3.90 | ppl    49.48\n",
            "| epoch  26 |  1000/ 1327 batches | lr 1.25 | ms/batch  6.71 | loss  3.98 | ppl    53.51\n",
            "| epoch  26 |  1200/ 1327 batches | lr 1.25 | ms/batch  6.68 | loss  3.84 | ppl    46.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time:  9.32s | valid loss  4.58 | valid ppl    97.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |   200/ 1327 batches | lr 1.25 | ms/batch  6.68 | loss  3.98 | ppl    53.37\n",
            "| epoch  27 |   400/ 1327 batches | lr 1.25 | ms/batch  6.65 | loss  3.97 | ppl    53.00\n",
            "| epoch  27 |   600/ 1327 batches | lr 1.25 | ms/batch  6.63 | loss  3.96 | ppl    52.38\n",
            "| epoch  27 |   800/ 1327 batches | lr 1.25 | ms/batch  6.62 | loss  3.89 | ppl    49.11\n",
            "| epoch  27 |  1000/ 1327 batches | lr 1.25 | ms/batch  6.79 | loss  3.97 | ppl    52.88\n",
            "| epoch  27 |  1200/ 1327 batches | lr 1.25 | ms/batch  6.84 | loss  3.83 | ppl    46.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time:  9.28s | valid loss  4.58 | valid ppl    97.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   200/ 1327 batches | lr 0.31 | ms/batch  6.76 | loss  3.98 | ppl    53.63\n",
            "| epoch  28 |   400/ 1327 batches | lr 0.31 | ms/batch  6.65 | loss  3.97 | ppl    52.99\n",
            "| epoch  28 |   600/ 1327 batches | lr 0.31 | ms/batch  6.67 | loss  3.95 | ppl    52.18\n",
            "| epoch  28 |   800/ 1327 batches | lr 0.31 | ms/batch  6.67 | loss  3.89 | ppl    48.74\n",
            "| epoch  28 |  1000/ 1327 batches | lr 0.31 | ms/batch  6.68 | loss  3.96 | ppl    52.33\n",
            "| epoch  28 |  1200/ 1327 batches | lr 0.31 | ms/batch  6.65 | loss  3.82 | ppl    45.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time:  9.21s | valid loss  4.58 | valid ppl    97.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   200/ 1327 batches | lr 0.31 | ms/batch  6.74 | loss  3.98 | ppl    53.36\n",
            "| epoch  29 |   400/ 1327 batches | lr 0.31 | ms/batch  6.85 | loss  3.96 | ppl    52.64\n",
            "| epoch  29 |   600/ 1327 batches | lr 0.31 | ms/batch  6.77 | loss  3.95 | ppl    52.00\n",
            "| epoch  29 |   800/ 1327 batches | lr 0.31 | ms/batch  6.68 | loss  3.88 | ppl    48.39\n",
            "| epoch  29 |  1000/ 1327 batches | lr 0.31 | ms/batch  6.65 | loss  3.95 | ppl    52.08\n",
            "| epoch  29 |  1200/ 1327 batches | lr 0.31 | ms/batch  6.65 | loss  3.81 | ppl    45.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time:  9.26s | valid loss  4.57 | valid ppl    97.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   200/ 1327 batches | lr 0.31 | ms/batch  6.67 | loss  3.97 | ppl    52.98\n",
            "| epoch  30 |   400/ 1327 batches | lr 0.31 | ms/batch  6.63 | loss  3.96 | ppl    52.58\n",
            "| epoch  30 |   600/ 1327 batches | lr 0.31 | ms/batch  6.69 | loss  3.95 | ppl    51.70\n",
            "| epoch  30 |   800/ 1327 batches | lr 0.31 | ms/batch  6.75 | loss  3.88 | ppl    48.39\n",
            "| epoch  30 |  1000/ 1327 batches | lr 0.31 | ms/batch  6.86 | loss  3.95 | ppl    52.14\n",
            "| epoch  30 |  1200/ 1327 batches | lr 0.31 | ms/batch  6.80 | loss  3.81 | ppl    45.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time:  9.26s | valid loss  4.57 | valid ppl    96.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   200/ 1327 batches | lr 0.31 | ms/batch  6.66 | loss  3.97 | ppl    52.82\n",
            "| epoch  31 |   400/ 1327 batches | lr 0.31 | ms/batch  6.68 | loss  3.96 | ppl    52.25\n",
            "| epoch  31 |   600/ 1327 batches | lr 0.31 | ms/batch  6.68 | loss  3.95 | ppl    51.79\n",
            "| epoch  31 |   800/ 1327 batches | lr 0.31 | ms/batch  6.66 | loss  3.88 | ppl    48.36\n",
            "| epoch  31 |  1000/ 1327 batches | lr 0.31 | ms/batch  6.68 | loss  3.95 | ppl    52.17\n",
            "| epoch  31 |  1200/ 1327 batches | lr 0.31 | ms/batch  6.71 | loss  3.81 | ppl    45.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time:  9.29s | valid loss  4.57 | valid ppl    96.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   200/ 1327 batches | lr 0.08 | ms/batch  6.81 | loss  3.96 | ppl    52.64\n",
            "| epoch  32 |   400/ 1327 batches | lr 0.08 | ms/batch  6.81 | loss  3.95 | ppl    52.18\n",
            "| epoch  32 |   600/ 1327 batches | lr 0.08 | ms/batch  6.69 | loss  3.94 | ppl    51.54\n",
            "| epoch  32 |   800/ 1327 batches | lr 0.08 | ms/batch  6.69 | loss  3.87 | ppl    48.13\n",
            "| epoch  32 |  1000/ 1327 batches | lr 0.08 | ms/batch  6.62 | loss  3.94 | ppl    51.64\n",
            "| epoch  32 |  1200/ 1327 batches | lr 0.08 | ms/batch  6.70 | loss  3.81 | ppl    44.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time:  9.27s | valid loss  4.57 | valid ppl    96.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   200/ 1327 batches | lr 0.08 | ms/batch  6.68 | loss  3.96 | ppl    52.67\n",
            "| epoch  33 |   400/ 1327 batches | lr 0.08 | ms/batch  6.64 | loss  3.96 | ppl    52.25\n",
            "| epoch  33 |   600/ 1327 batches | lr 0.08 | ms/batch  6.78 | loss  3.94 | ppl    51.53\n",
            "| epoch  33 |   800/ 1327 batches | lr 0.08 | ms/batch  6.79 | loss  3.87 | ppl    48.12\n",
            "| epoch  33 |  1000/ 1327 batches | lr 0.08 | ms/batch  6.82 | loss  3.95 | ppl    51.91\n",
            "| epoch  33 |  1200/ 1327 batches | lr 0.08 | ms/batch  6.68 | loss  3.81 | ppl    44.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time:  9.27s | valid loss  4.57 | valid ppl    96.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |   200/ 1327 batches | lr 0.08 | ms/batch  6.68 | loss  3.96 | ppl    52.52\n",
            "| epoch  34 |   400/ 1327 batches | lr 0.08 | ms/batch  6.67 | loss  3.95 | ppl    52.09\n",
            "| epoch  34 |   600/ 1327 batches | lr 0.08 | ms/batch  6.64 | loss  3.94 | ppl    51.64\n",
            "| epoch  34 |   800/ 1327 batches | lr 0.08 | ms/batch  6.69 | loss  3.88 | ppl    48.22\n",
            "| epoch  34 |  1000/ 1327 batches | lr 0.08 | ms/batch  6.67 | loss  3.95 | ppl    51.71\n",
            "| epoch  34 |  1200/ 1327 batches | lr 0.08 | ms/batch  6.71 | loss  3.81 | ppl    45.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time:  9.30s | valid loss  4.57 | valid ppl    96.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   200/ 1327 batches | lr 0.08 | ms/batch  6.84 | loss  3.96 | ppl    52.55\n",
            "| epoch  35 |   400/ 1327 batches | lr 0.08 | ms/batch  6.69 | loss  3.95 | ppl    52.05\n",
            "| epoch  35 |   600/ 1327 batches | lr 0.08 | ms/batch  6.65 | loss  3.94 | ppl    51.49\n",
            "| epoch  35 |   800/ 1327 batches | lr 0.08 | ms/batch  6.66 | loss  3.88 | ppl    48.19\n",
            "| epoch  35 |  1000/ 1327 batches | lr 0.08 | ms/batch  6.63 | loss  3.95 | ppl    51.72\n",
            "| epoch  35 |  1200/ 1327 batches | lr 0.08 | ms/batch  6.67 | loss  3.81 | ppl    45.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time:  9.22s | valid loss  4.57 | valid ppl    96.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   200/ 1327 batches | lr 0.08 | ms/batch  6.65 | loss  3.96 | ppl    52.48\n",
            "| epoch  36 |   400/ 1327 batches | lr 0.08 | ms/batch  6.82 | loss  3.95 | ppl    52.13\n",
            "| epoch  36 |   600/ 1327 batches | lr 0.08 | ms/batch  6.84 | loss  3.94 | ppl    51.38\n",
            "| epoch  36 |   800/ 1327 batches | lr 0.08 | ms/batch  6.82 | loss  3.87 | ppl    48.16\n",
            "| epoch  36 |  1000/ 1327 batches | lr 0.08 | ms/batch  6.70 | loss  3.95 | ppl    51.83\n",
            "| epoch  36 |  1200/ 1327 batches | lr 0.08 | ms/batch  6.71 | loss  3.81 | ppl    45.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time:  9.30s | valid loss  4.57 | valid ppl    96.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |   200/ 1327 batches | lr 0.08 | ms/batch  6.66 | loss  3.96 | ppl    52.48\n",
            "| epoch  37 |   400/ 1327 batches | lr 0.08 | ms/batch  6.66 | loss  3.95 | ppl    51.90\n",
            "| epoch  37 |   600/ 1327 batches | lr 0.08 | ms/batch  6.66 | loss  3.94 | ppl    51.51\n",
            "| epoch  37 |   800/ 1327 batches | lr 0.08 | ms/batch  6.67 | loss  3.87 | ppl    48.07\n",
            "| epoch  37 |  1000/ 1327 batches | lr 0.08 | ms/batch  6.77 | loss  3.94 | ppl    51.56\n",
            "| epoch  37 |  1200/ 1327 batches | lr 0.08 | ms/batch  6.84 | loss  3.81 | ppl    45.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time:  9.30s | valid loss  4.57 | valid ppl    96.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   200/ 1327 batches | lr 0.02 | ms/batch  6.73 | loss  3.96 | ppl    52.43\n",
            "| epoch  38 |   400/ 1327 batches | lr 0.02 | ms/batch  6.68 | loss  3.95 | ppl    51.78\n",
            "| epoch  38 |   600/ 1327 batches | lr 0.02 | ms/batch  6.69 | loss  3.94 | ppl    51.50\n",
            "| epoch  38 |   800/ 1327 batches | lr 0.02 | ms/batch  6.66 | loss  3.88 | ppl    48.18\n",
            "| epoch  38 |  1000/ 1327 batches | lr 0.02 | ms/batch  6.71 | loss  3.94 | ppl    51.66\n",
            "| epoch  38 |  1200/ 1327 batches | lr 0.02 | ms/batch  6.71 | loss  3.80 | ppl    44.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time:  9.23s | valid loss  4.57 | valid ppl    96.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   200/ 1327 batches | lr 0.02 | ms/batch  6.76 | loss  3.96 | ppl    52.43\n",
            "| epoch  39 |   400/ 1327 batches | lr 0.02 | ms/batch  6.83 | loss  3.95 | ppl    52.12\n",
            "| epoch  39 |   600/ 1327 batches | lr 0.02 | ms/batch  6.82 | loss  3.94 | ppl    51.25\n",
            "| epoch  39 |   800/ 1327 batches | lr 0.02 | ms/batch  6.74 | loss  3.87 | ppl    48.01\n",
            "| epoch  39 |  1000/ 1327 batches | lr 0.02 | ms/batch  6.67 | loss  3.94 | ppl    51.48\n",
            "| epoch  39 |  1200/ 1327 batches | lr 0.02 | ms/batch  6.64 | loss  3.80 | ppl    44.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time:  9.28s | valid loss  4.57 | valid ppl    96.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   200/ 1327 batches | lr 0.02 | ms/batch  6.67 | loss  3.96 | ppl    52.55\n",
            "| epoch  40 |   400/ 1327 batches | lr 0.02 | ms/batch  6.64 | loss  3.95 | ppl    51.74\n",
            "| epoch  40 |   600/ 1327 batches | lr 0.02 | ms/batch  6.68 | loss  3.94 | ppl    51.44\n",
            "| epoch  40 |   800/ 1327 batches | lr 0.02 | ms/batch  6.73 | loss  3.87 | ppl    47.87\n",
            "| epoch  40 |  1000/ 1327 batches | lr 0.02 | ms/batch  6.81 | loss  3.95 | ppl    51.78\n",
            "| epoch  40 |  1200/ 1327 batches | lr 0.02 | ms/batch  6.83 | loss  3.81 | ppl    45.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time:  9.26s | valid loss  4.57 | valid ppl    96.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  4.53 | test ppl    92.57\n",
            "=========================================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-34c8641bc277>\u001b[0m in \u001b[0;36m<cell line: 272>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0mresults_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#add test ppl to result matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx_export\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: plot_results() missing 1 required positional argument: 'file_name'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(results_m, 'first')"
      ],
      "metadata": {
        "id": "YbgOMTD2X41i",
        "outputId": "ee7bbadd-2bb1-4096-8902-1f1ef52d8502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# other model start\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xG61dP7N_rO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "        self.counter = Counter()\n",
        "        self.total = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        token_id = self.word2idx[word]\n",
        "        self.counter[token_id] += 1\n",
        "        self.total += 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r') as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r') as f:\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "\n",
        "        return ids\n"
      ],
      "metadata": {
        "id": "POJC3frTFemX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoH9gEEl2CYI",
        "outputId": "5e1f0f50-2740-4c65-d19b-46e9b7e379f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f225069d150>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get first 100 words\n",
        "\n",
        "for word_index in range(100):\n",
        "  print(f'word index = {word_index}, word = {corpus.dictionary.idx2word[word_index]}')"
      ],
      "metadata": {
        "id": "zil5zgoZ0R3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume `data` is a 1D tensor containing your entire dataset\n",
        "batch_size = 20\n",
        "seq_length = 35\n",
        "data = corpus.train\n",
        "num_batches = data.size(0) // (batch_size * seq_length)\n",
        "\n",
        "# Discard any extra data that won't fit into full batches\n",
        "data = data[:num_batches * batch_size * seq_length]\n",
        "\n",
        "# Reshape data into [batch_size, -1], maintaining continuity across batches\n",
        "data = data.view(batch_size, -1)\n",
        "\n",
        "print(data[:, 0:35])\n",
        "print()"
      ],
      "metadata": {
        "id": "dXMud5TmqXna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test data\n",
        "\n",
        "def test_model(model_):\n",
        "  model_.eval()  \n",
        "\n",
        "  # Define a variable to hold the total loss of the model on the test data\n",
        "  total_loss = 0\n",
        "  total_samples_count = 0\n",
        "\n",
        "  with torch.no_grad():  # turn off gradients, since we are in test mode\n",
        "      for inputs in test_dataloader:\n",
        "          inputs = inputs.to(device)\n",
        "\n",
        "          targets = inputs[:, 1:].contiguous()\n",
        "          inputs = inputs[:, :-1].contiguous()\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = model(inputs)\n",
        "          outputs = outputs.view(-1, outputs.size(-1))\n",
        "\n",
        "          targets = targets.view(-1)\n",
        "          targets = targets.to(device)\n",
        "\n",
        "          loss = loss_function(outputs, targets)\n",
        "\n",
        "          #total_loss += loss.item()\n",
        "\n",
        "          batch_size = inputs.size(0)\n",
        "          total_loss += loss.item() * batch_size\n",
        "          total_samples_count += batch_size \n",
        "\n",
        "  # Compute the average loss over the entire test data\n",
        "  #average_loss = total_loss / len(test_dataloader)\n",
        "\n",
        "  # Compute perplexity based on the average loss\n",
        "  #test_perplexity = math.exp(average_loss)\n",
        "  test_perplexity = np.exp(total_loss / total_samples_count)\n",
        "\n",
        "  results_m.add_result(test_perplexity, 'test') \n",
        "\n",
        "  #print(f\"Test result, Average Loss: {average_loss}, Test Perplexity: {test_perplexity}\")\n",
        "  print(f\"Test results  - Loss: {loss.item()}, Perplexity: {test_perplexity.item()}\")\n",
        "  print('-------------------------------------------------------------------------')\n"
      ],
      "metadata": {
        "id": "RKrJ1foMXKYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = Corpus('data/ptb')"
      ],
      "metadata": {
        "id": "p_oWKWhwye69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyper params\n",
        "model_type = \"LSTM\" #change to GRU \n",
        "total_epochs = 15\n",
        "embedding_dim = 1500\n",
        "hidden_dim = 200\n",
        "drop_out = 0.4\n",
        "learning_rate = 1e-2\n",
        "# hyper params\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(1111)\n",
        "\n",
        "# First, let's define a custom Dataset to read the vectors of words\n",
        "class PTBDataset(Dataset):\n",
        "    def __init__(self, data_, sequence_length_=35):\n",
        "        # In practice, `data` should be a list of integers representing the words in the PTB data set.\n",
        "        # self.data = [torch.tensor(item, dtype=torch.long) for item in data]\n",
        "        self.data = data_\n",
        "        self.sequence_length = sequence_length_ + 1\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.data) // self.sequence_length\n",
        "   \n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx * self.sequence_length: (idx + 1) * self.sequence_length]\n",
        "        return x\n",
        "\n",
        "# Collate function to pad sequences in the same batch to the same length\n",
        "def collate(batch):\n",
        "    return pad_sequence(batch, batch_first=True)\n",
        "\n",
        "# Create a DataLoader\n",
        "train_data = corpus.train  \n",
        "data = train_data\n",
        "print(data)\n",
        "dataset = PTBDataset(data)\n",
        "# dataloader = DataLoader(dataset, batch_size=20, collate_fn=collate, shuffle=True)\n",
        "dataloader = DataLoader(dataset, batch_size=20, collate_fn=collate, shuffle=False)\n",
        "\n",
        "# Prepare test data\n",
        "test_data = corpus.test\n",
        "test_dataset = PTBDataset(test_data)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, collate_fn=collate, shuffle=False)\n",
        "\n",
        "# Now let's define the LSTM language model\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, model_type_, vocab_size, embedding_dim, hidden_dim, nlayers = 2, dropout_ = 0):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.model_type = model_type_\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.nlayers = nlayers        \n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.dropout = dropout_\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, num_layers = nlayers, dropout = dropout_)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, num_layers = nlayers, dropout = dropout_)\n",
        "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.hidden = self.init_hidden(20)  \n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters())\n",
        "        if self.model_type == 'LSTM':\n",
        "            return (weight.new_zeros(self.nlayers, batch_size, self.hidden_dim),\n",
        "                    weight.new_zeros(self.nlayers, batch_size, self.hidden_dim))\n",
        "        else:  # GRU\n",
        "            return weight.new_zeros(self.nlayers, batch_size, self.hidden_dim)        \n",
        "\n",
        "    def repackage_hidden(self, h):\n",
        "        \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "        if isinstance(h, torch.Tensor):\n",
        "            return h.detach()\n",
        "        else:\n",
        "            return tuple(self.repackage_hidden(v) for v in h)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.hidden = self.repackage_hidden(self.hidden)\n",
        "\n",
        "        x = self.embeddings(x)\n",
        "        if model_type == 'LSTM':\n",
        "          x, _ = self.lstm(x)\n",
        "        elif model_type == 'GRU':\n",
        "          x, _ = self.gru(x)  \n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "vocab_size = len(corpus.dictionary)  \n",
        "\n",
        "model = LanguageModel(model_type, vocab_size, embedding_dim, hidden_dim, dropout_ = drop_out)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Define a loss function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define an optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "results_m = Result_Matrix(f'{model.model_type}. Dropout = {model.dropout != 0}') \n",
        "\n",
        "for epoch in range(total_epochs):  \n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_samples_count = 0\n",
        "    for inputs in dataloader:\n",
        "        inputs = inputs.to(device)        \n",
        "\n",
        "        targets = inputs[:, 1:].contiguous()\n",
        "        inputs = inputs[:, :-1].contiguous()\n",
        "\n",
        "        if (10 < epoch):\n",
        "          #print(f'inputs = {inputs[3, :]}')\n",
        "          words_index = inputs[0, :].cpu()\n",
        "          print('model input', end = ' ')\n",
        "          for word_index in words_index:\n",
        "            print(corpus.dictionary.idx2word[word_index], end = ' ')\n",
        "\n",
        "          print('\\n')\n",
        "\n",
        "          print('model target:', end = ' ')\n",
        "          words_index = targets[0, :].cpu()\n",
        "          for word_index in words_index:\n",
        "            print(corpus.dictionary.idx2word[word_index], end = ' ')\n",
        "          print('\\n')\n",
        "\n",
        "        # Forward pass\n",
        "        print(inputs.shape)\n",
        "        keysin = input('press any key')\n",
        "        outputs = model(inputs)\n",
        "\n",
        "\n",
        "        if (10 < epoch):\n",
        "          # Get class indices\n",
        "          _, predicted_indices = torch.max(outputs[0, :, :], -1)\n",
        "\n",
        "          # Convert indices to words\n",
        "          predicted_words = [corpus.dictionary.idx2word[index] for index in predicted_indices]\n",
        "\n",
        "          # Print predicted words\n",
        "          print('model output:', end = '')\n",
        "          for word in predicted_words:\n",
        "              print(word, end=' ')\n",
        "          print('\\n')        \n",
        "\n",
        "          #user_inp = input('press any key')\n",
        "\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.size(-1))\n",
        "\n",
        "        targets = targets.to(device).view(-1)\n",
        "       \n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        batch_size = inputs.size(0)\n",
        "        total_loss += loss.item() * batch_size\n",
        "        total_samples_count += batch_size        \n",
        "              \n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Print gradients\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "              if torch.any(1e3 < torch.abs(param.grad)):\n",
        "                print(f\"Gradient of {name} is {param.grad.data}\")        \n",
        "\n",
        "        optimizer.step()\n",
        "   \n",
        "    # Calculate perplexity\n",
        "    perplexity = np.exp(total_loss / total_samples_count)\n",
        "    results_m.add_result(perplexity, 'train')  \n",
        "\n",
        "    print(f\"Epoch: {epoch + 1}\")\n",
        "    print(f\"Train results - Loss: {loss.item()}, Perplexity: {perplexity.item()}\")\n",
        "\n",
        "    test_model(model)\n",
        "\n",
        "plot_results(results_m, f'{model.model_type} Dropout = {model.dropout != 0}.jpg')"
      ],
      "metadata": {
        "id": "3ntuJXBfM6Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.shape"
      ],
      "metadata": {
        "id": "9cR2tzLDERK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "york ruling that all manufacturers of an <unk> drug are liable for injuries or deaths if the actual maker is n't known <eos> revco received a $ N million takeover offer from texas financier robert \n",
        "\n",
        "\n",
        "\n",
        "york ruling that all manufacturers of an <unk> drug are liable for injuries or deaths if the actual maker is n't known \n",
        " revco received a $ N million takeover offer from texas financier robert bass and acadia partners"
      ],
      "metadata": {
        "id": "pyFmYSmJ5dSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_softmax = F.softmax(outputs[5])\n",
        "\n",
        "\n",
        "y = output_softmax.cpu()\n",
        "y = y.detach().numpy()\n",
        "print(np.sum(y))\n",
        "print(y.shape)\n",
        "plt.plot(np.arange(1,len(y)+1), y, label = 'outputs', color = 'blue')\n",
        "\n",
        "\n",
        "#plt.legend()\n",
        "plt.title(f'outputs at epoch = {epoch}')\n",
        "#plt.ylim((0.75, 1))\n",
        "plt.xlabel('output')\n",
        "plt.ylabel('value')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r8R7HHjIROCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(results_m, f'{model.model_type} Dropout = {model.dropout != 0}.jpg')"
      ],
      "metadata": {
        "id": "lwrJDdAFwd3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = torch.randn(3, 5, requires_grad=True)\n",
        "print(x1)\n"
      ],
      "metadata": {
        "id": "vsUB5BbpAwOL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}