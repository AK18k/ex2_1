{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AK18k/ex2_1/blob/main/ex2_latenight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AK18k/ex2_1\n",
        "\n"
      ],
      "metadata": {
        "id": "K1j_jwEwF_MG",
        "outputId": "5dd23ec5-e81f-423f-efd1-47ab4d1f1bde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ex2_1' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "DATA_PATH = '/content/ex2_1/data/ptb'\n",
        "PATH = '/content/ex2_1'\n",
        "os.chdir('/content/ex2_1')\n",
        "!ls"
      ],
      "metadata": {
        "id": "1TkQzsA-FqiH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfb5c103-d870-46e0-a969-909589eca730"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data\t\t     main.ipynb\t\t\t\t    test_result.png\n",
            "ex2_1\t\t     README.md\n",
            "ex2_latenight.ipynb  test_and_train_results_no_dropout.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "GW4fCoGkFZvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'device is {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_962w4A9wmZa",
        "outputId": "2b7912b0-9d98-474b-d35a-632397d8f9dd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device is cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "        self.counter = Counter()\n",
        "        self.total = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        token_id = self.word2idx[word]\n",
        "        self.counter[token_id] += 1\n",
        "        self.total += 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r') as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r') as f:\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "\n",
        "        return ids\n"
      ],
      "metadata": {
        "id": "POJC3frTFemX"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Result_Matrix:\n",
        "    def __init__(self, model_type):\n",
        "      self.model_name = model_type\n",
        "      self.nepochs = 0\n",
        "      self.train_ppl = np.array([])\n",
        "      self.val_ppl = np.array([])\n",
        "      self.test_ppl = np.array([])\n",
        "\n",
        "    def add_result(self, result, result_type = 'train'):\n",
        "      if result_type == 'train':\n",
        "        self.train_ppl = np.append(self.train_ppl, result)\n",
        "      elif result_type == 'val':\n",
        "        self.val_ppl = np.append(self.val_ppl, result)\n",
        "      elif result_type == 'test':\n",
        "        self.test_ppl = np.append(self.test_ppl, result)\n",
        "\n",
        "    def get_results(self, result_type = 'train'):\n",
        "      if result_type == 'train':\n",
        "        return(self.train_ppl)\n",
        "      elif result_type == 'val':\n",
        "        return(self.val_ppl)\n",
        "      elif result_type == 'test':\n",
        "        return(self.test_ppl) \n",
        "\n",
        "\n",
        "def plot_results(result_matrix, file_name):\n",
        "  plt.figure()\n",
        "  y = result_matrix.get_results('train')\n",
        "  plt.plot(np.arange(1,len(y)+1), y, label = 'train', color = 'blue')\n",
        "  y = result_matrix.get_results('val')\n",
        "  plt.plot(np.arange(1,len(y)+1), y, label = 'val', color = 'green')\n",
        "  y = result_matrix.get_results('test')\n",
        "  plt.plot(np.arange(1,len(y)+1), y, label = 'test', color = 'red')\n",
        "  plt.legend()\n",
        "  plt.title(f'Model Type = {result_matrix.model_name}')\n",
        "  #plt.ylim((0.75, 1))\n",
        "  plt.xlabel('eoch')\n",
        "  plt.ylabel('perplexity')\n",
        "  plt.grid()\n",
        "  plt.show() \n",
        "\n",
        "  plt.savefig(file_name) \n"
      ],
      "metadata": {
        "id": "fB5XPLnpFhzI"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test data\n",
        "\n",
        "def test_model(model_):\n",
        "  model_.eval()  \n",
        "\n",
        "  test_data = corpus.test\n",
        "  test_dataset = PTBDataset(test_data)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=20, collate_fn=collate, shuffle=False)\n",
        "\n",
        "  # Define a variable to hold the total loss of the model on the test data\n",
        "  total_loss = 0\n",
        "\n",
        "  with torch.no_grad():  # turn off gradients, since we are in test mode\n",
        "      for inputs in test_dataloader:\n",
        "          inputs = inputs.to(device)\n",
        "\n",
        "          targets = inputs[:, 1:].contiguous()\n",
        "          inputs = inputs[:, :-1].contiguous()\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = model(inputs)\n",
        "          outputs = outputs.view(-1, outputs.size(-1))\n",
        "\n",
        "          targets = targets.view(-1)\n",
        "\n",
        "          loss = loss_function(outputs, targets)\n",
        "\n",
        "          total_loss += loss.item()\n",
        "\n",
        "  # Compute the average loss over the entire test data\n",
        "  average_loss = total_loss / len(test_dataloader)\n",
        "\n",
        "  # Compute perplexity based on the average loss\n",
        "  test_perplexity = math.exp(average_loss)\n",
        "\n",
        "  results_m.add_result(test_perplexity, 'test') \n",
        "\n",
        "  print(f\"Test result, Average Loss: {average_loss}, Test Perplexity: {test_perplexity}\")\n"
      ],
      "metadata": {
        "id": "RKrJ1foMXKYh"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyper params\n",
        "total_epochs = 100\n",
        "embedding_dim = 700\n",
        "hidden_dim = 200\n",
        "drop_out = 0.3\n",
        "# hyper params\n",
        "\n",
        "# First, let's define a custom Dataset to read the vectors of words\n",
        "class PTBDataset(Dataset):\n",
        "    def __init__(self, data_, sequence_length_=35):\n",
        "        # In practice, `data` should be a list of integers representing the words in the PTB data set.\n",
        "        # self.data = [torch.tensor(item, dtype=torch.long) for item in data]\n",
        "        self.data = data_\n",
        "        self.sequence_length = sequence_length_ + 1\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.data) // self.sequence_length\n",
        "   \n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx * self.sequence_length: (idx + 1) * self.sequence_length]\n",
        "        return x\n",
        "\n",
        "# Collate function to pad sequences in the same batch to the same length\n",
        "def collate(batch):\n",
        "    return pad_sequence(batch, batch_first=True)\n",
        "\n",
        "# Create a DataLoader\n",
        "corpus = Corpus('data/ptb')\n",
        "train_data = corpus.train  \n",
        "data = train_data\n",
        "print(data)\n",
        "dataset = PTBDataset(data)\n",
        "# dataloader = DataLoader(dataset, batch_size=20, collate_fn=collate, shuffle=True)\n",
        "dataloader = DataLoader(dataset, batch_size=20, collate_fn=collate, shuffle=False)\n",
        "\n",
        "# Now let's define the LSTM language model\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, nlayers = 2, dropout_ = 0):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, num_layers = nlayers, dropout = dropout_)\n",
        "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "vocab_size = len(corpus.dictionary)  \n",
        "\n",
        "model = LanguageModel(vocab_size, embedding_dim, hidden_dim, dropout_ = drop_out)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Define a loss function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define an optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "results_m = Result_Matrix('LSTM') \n",
        "\n",
        "for epoch in range(total_epochs):  # replace 10 with the number of epochs you want to train\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_samples_count = 0\n",
        "    for inputs in dataloader:\n",
        "        inputs = inputs.to(device)        \n",
        "\n",
        "        targets = inputs[:, 1:].contiguous()\n",
        "        inputs = inputs[:, :-1].contiguous()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        outputs = outputs.view(-1, outputs.size(-1))\n",
        "\n",
        "        targets = targets.to(device).view(-1)\n",
        "       \n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        batch_size = inputs.size(0)\n",
        "        total_loss += loss.item() * batch_size\n",
        "        total_samples_count += batch_size        \n",
        "              \n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "   \n",
        "    # Calculate perplexity\n",
        "    perplexity = np.exp(total_loss / total_samples_count)\n",
        "    results_m.add_result(perplexity, 'train')  \n",
        "    print(f\"Train results, Epoch: {epoch+1}, Loss: {loss.item()}, Perplexity: {perplexity.item()}\")\n",
        "\n",
        "    test_model(model)\n",
        "\n",
        "plot_results(results_m, 'result.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3ntuJXBfM6Ym",
        "outputId": "8fa39322-1ebb-4203-f7b1-205a6822e988"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0,  1,  2,  ..., 39, 26, 24])\n",
            "Train results, Epoch: 1, Loss: 6.196535587310791, Perplexity: 341.09113831574984\n",
            "Test result, Average Loss: 5.380490522799285, Test Perplexity: 217.12875591241442\n",
            "Train results, Epoch: 2, Loss: 5.239674091339111, Perplexity: 207.34040499741\n",
            "Test result, Average Loss: 5.254757545305335, Test Perplexity: 191.47505619977377\n",
            "Train results, Epoch: 3, Loss: 4.822380065917969, Perplexity: 177.49887108570562\n",
            "Test result, Average Loss: 5.215389075486557, Test Perplexity: 184.08342939806317\n",
            "Train results, Epoch: 4, Loss: 4.646462917327881, Perplexity: 158.76174522626656\n",
            "Test result, Average Loss: 5.208755422675091, Test Perplexity: 182.86632522560492\n",
            "Train results, Epoch: 5, Loss: 4.477176666259766, Perplexity: 147.1195674028955\n",
            "Test result, Average Loss: 5.215859624613886, Test Perplexity: 184.17007007787174\n",
            "Train results, Epoch: 6, Loss: 4.416530609130859, Perplexity: 139.04285355553895\n",
            "Test result, Average Loss: 5.233546990933625, Test Perplexity: 187.45653232487473\n",
            "Train results, Epoch: 7, Loss: 4.172442436218262, Perplexity: 133.09897125924337\n",
            "Test result, Average Loss: 5.259087253653485, Test Perplexity: 192.30588467363597\n",
            "Train results, Epoch: 8, Loss: 4.153368949890137, Perplexity: 128.28166969423197\n",
            "Test result, Average Loss: 5.282591110727061, Test Perplexity: 196.87935121143107\n",
            "Train results, Epoch: 9, Loss: 3.881403923034668, Perplexity: 124.28022137059081\n",
            "Test result, Average Loss: 5.3149298045946205, Test Perplexity: 203.35023837542576\n",
            "Train results, Epoch: 10, Loss: 3.9894988536834717, Perplexity: 121.61030271206492\n",
            "Test result, Average Loss: 5.351934563595315, Test Perplexity: 211.0161273714621\n",
            "Train results, Epoch: 11, Loss: 3.8406333923339844, Perplexity: 119.42679121774454\n",
            "Test result, Average Loss: 5.393865440202796, Test Perplexity: 220.0523428728725\n",
            "Train results, Epoch: 12, Loss: 3.8505969047546387, Perplexity: 117.59385835887718\n",
            "Test result, Average Loss: 5.431148549784785, Test Perplexity: 228.41143673179684\n",
            "Train results, Epoch: 13, Loss: 3.757206916809082, Perplexity: 115.49541911695744\n",
            "Test result, Average Loss: 5.460410829212354, Test Perplexity: 235.19402909758136\n",
            "Train results, Epoch: 14, Loss: 3.6572422981262207, Perplexity: 113.77041773175239\n",
            "Test result, Average Loss: 5.501596960814103, Test Perplexity: 245.0830078746733\n",
            "Train results, Epoch: 15, Loss: 3.7680370807647705, Perplexity: 112.21601545688264\n",
            "Test result, Average Loss: 5.543083329822706, Test Perplexity: 255.4644675766998\n",
            "Train results, Epoch: 16, Loss: 3.7026431560516357, Perplexity: 111.29347375625268\n",
            "Test result, Average Loss: 5.584413777226987, Test Perplexity: 266.24415859322244\n",
            "Train results, Epoch: 17, Loss: 3.678640365600586, Perplexity: 110.31633175141916\n",
            "Test result, Average Loss: 5.6180055265841276, Test Perplexity: 275.33967756309744\n",
            "Train results, Epoch: 18, Loss: 3.5237233638763428, Perplexity: 109.26221876167504\n",
            "Test result, Average Loss: 5.659547718711521, Test Perplexity: 287.0187999629152\n",
            "Train results, Epoch: 19, Loss: 3.593858242034912, Perplexity: 108.41263411289245\n",
            "Test result, Average Loss: 5.693705749511719, Test Perplexity: 296.99216248399244\n",
            "Train results, Epoch: 20, Loss: 3.578840494155884, Perplexity: 107.37146862852676\n",
            "Test result, Average Loss: 5.725306148114412, Test Perplexity: 306.5270935716616\n",
            "Train results, Epoch: 21, Loss: 3.605879783630371, Perplexity: 106.88581691710901\n",
            "Test result, Average Loss: 5.766148544394452, Test Perplexity: 319.3055701329017\n",
            "Train results, Epoch: 22, Loss: 3.573833703994751, Perplexity: 106.23654639339456\n",
            "Test result, Average Loss: 5.821639782449473, Test Perplexity: 337.52506777710715\n",
            "Train results, Epoch: 23, Loss: 3.53544545173645, Perplexity: 105.86948833282416\n",
            "Test result, Average Loss: 5.830224462177442, Test Perplexity: 340.4350852967976\n",
            "Train results, Epoch: 24, Loss: 3.765190839767456, Perplexity: 105.5507503388766\n",
            "Test result, Average Loss: 5.857966093395067, Test Perplexity: 350.0115288089348\n",
            "Train results, Epoch: 25, Loss: 3.458693027496338, Perplexity: 104.98849299482276\n",
            "Test result, Average Loss: 5.9038298876389215, Test Perplexity: 366.438200964273\n",
            "Train results, Epoch: 26, Loss: 3.4219491481781006, Perplexity: 104.5839628321059\n",
            "Test result, Average Loss: 5.9193472654923145, Test Perplexity: 372.1687072175128\n",
            "Train results, Epoch: 27, Loss: 3.967336654663086, Perplexity: 104.12520740227002\n",
            "Test result, Average Loss: 5.950955152511597, Test Perplexity: 384.12005713471393\n",
            "Train results, Epoch: 28, Loss: 3.2856550216674805, Perplexity: 104.18117307063847\n",
            "Test result, Average Loss: 5.985997795022052, Test Perplexity: 397.8192653644408\n",
            "Train results, Epoch: 29, Loss: 3.5832104682922363, Perplexity: 104.03525779098345\n",
            "Test result, Average Loss: 6.02388363091842, Test Perplexity: 413.1801229437674\n",
            "Train results, Epoch: 30, Loss: 3.388944387435913, Perplexity: 103.65134948952003\n",
            "Test result, Average Loss: 6.044688403088114, Test Perplexity: 421.8662847690165\n",
            "Train results, Epoch: 31, Loss: 3.3848941326141357, Perplexity: 103.9224625424999\n",
            "Test result, Average Loss: 6.08454270362854, Test Perplexity: 439.0190049642937\n",
            "Train results, Epoch: 32, Loss: 3.4036705493927, Perplexity: 103.57123960815498\n",
            "Test result, Average Loss: 6.1214747345965845, Test Perplexity: 455.5359940739576\n",
            "Train results, Epoch: 33, Loss: 3.219013214111328, Perplexity: 103.07770322132494\n",
            "Test result, Average Loss: 6.155847213579261, Test Perplexity: 471.4661057932767\n",
            "Train results, Epoch: 34, Loss: 3.3251307010650635, Perplexity: 102.632577612364\n",
            "Test result, Average Loss: 6.18881369051726, Test Perplexity: 487.2677128970914\n",
            "Train results, Epoch: 35, Loss: 3.1867918968200684, Perplexity: 102.57483878141731\n",
            "Test result, Average Loss: 6.218977673157402, Test Perplexity: 502.18956762347227\n",
            "Train results, Epoch: 36, Loss: 3.4549765586853027, Perplexity: 102.2716521557804\n",
            "Test result, Average Loss: 6.224827515560648, Test Perplexity: 505.1359068585862\n",
            "Train results, Epoch: 37, Loss: 3.1421189308166504, Perplexity: 102.35131814230847\n",
            "Test result, Average Loss: 6.268575002836145, Test Perplexity: 527.7248354258237\n",
            "Train results, Epoch: 38, Loss: 3.346421718597412, Perplexity: 102.71715015870389\n",
            "Test result, Average Loss: 6.289235001024992, Test Perplexity: 538.7410350628802\n",
            "Train results, Epoch: 39, Loss: 3.111748456954956, Perplexity: 102.4022202663772\n",
            "Test result, Average Loss: 6.321651887893677, Test Perplexity: 556.4914951745891\n",
            "Train results, Epoch: 40, Loss: 3.3634848594665527, Perplexity: 101.83770282072481\n",
            "Test result, Average Loss: 6.358179660465407, Test Perplexity: 577.1947091583659\n",
            "Train results, Epoch: 41, Loss: 3.3282310962677, Perplexity: 102.1622771573067\n",
            "Test result, Average Loss: 6.365939656547878, Test Perplexity: 581.6911615027951\n",
            "Train results, Epoch: 42, Loss: 3.4837985038757324, Perplexity: 102.07868788530246\n",
            "Test result, Average Loss: 6.3773010792939555, Test Perplexity: 588.3376861892989\n",
            "Train results, Epoch: 43, Loss: 3.2872090339660645, Perplexity: 102.03667796244555\n",
            "Test result, Average Loss: 6.416879647711049, Test Perplexity: 612.0901941471944\n",
            "Train results, Epoch: 44, Loss: 3.1646320819854736, Perplexity: 101.6358884319438\n",
            "Test result, Average Loss: 6.424896914026012, Test Perplexity: 617.0172084443966\n",
            "Train results, Epoch: 45, Loss: 3.159240245819092, Perplexity: 101.549691090303\n",
            "Test result, Average Loss: 6.437188200328661, Test Perplexity: 624.6479433125423\n",
            "Train results, Epoch: 46, Loss: 3.1682209968566895, Perplexity: 101.15473787887214\n",
            "Test result, Average Loss: 6.463563008930372, Test Perplexity: 641.3420980936659\n",
            "Train results, Epoch: 47, Loss: 3.111842155456543, Perplexity: 101.35240489468194\n",
            "Test result, Average Loss: 6.490015581379766, Test Perplexity: 658.5336239977661\n",
            "Train results, Epoch: 48, Loss: 3.012228012084961, Perplexity: 101.39716812615754\n",
            "Test result, Average Loss: 6.52320905353712, Test Perplexity: 680.7594774560705\n",
            "Train results, Epoch: 49, Loss: 3.2338004112243652, Perplexity: 101.7086428104912\n",
            "Test result, Average Loss: 6.555279078690902, Test Perplexity: 702.9452995050426\n",
            "Train results, Epoch: 50, Loss: 3.0045440196990967, Perplexity: 101.4167896027003\n",
            "Test result, Average Loss: 6.573619013247282, Test Perplexity: 715.9562152800446\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-b8f69bda16d6>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(results_m, 'lstm_with_dropout.png')"
      ],
      "metadata": {
        "id": "lwrJDdAFwd3A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}