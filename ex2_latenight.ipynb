{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AK18k/ex2_1/blob/avis_lab/ex2_latenight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#    Clone the GIT repository including py and data files     +\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "!git clone -b avis_lab https://github.com/AK18k/ex2_1\n",
        "\n"
      ],
      "metadata": {
        "id": "K1j_jwEwF_MG",
        "outputId": "2b4b1028-3a01-40ad-9184-9218da74456e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ex2_1'...\n",
            "remote: Enumerating objects: 138, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 138 (delta 12), reused 0 (delta 0), pack-reused 112\u001b[K\n",
            "Receiving objects: 100% (138/138), 2.41 MiB | 3.54 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#    Mount Colab drive to access files on the cloud           +\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PATH = '/content/ex2_1'\n",
        "DATA_PATH = '/content/ex2_1/data/ptb'\n",
        "os.chdir('/content/ex2_1')\n",
        "!ls"
      ],
      "metadata": {
        "id": "1TkQzsA-FqiH",
        "outputId": "1e1eefb9-3dae-49c7-f165-250e6079acb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "data\t ex2_latenight.ipynb  main.ipynb  README.md  train.txt\n",
            "data.py  generate.py\t      model.py\t  test.txt   valid.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#    Import python packages                                   +\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import numpy as np\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import torch.onnx\n",
        "\n",
        "import data\n",
        "import model\n",
        "\n"
      ],
      "metadata": {
        "id": "GW4fCoGkFZvA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'device is {device}')"
      ],
      "metadata": {
        "id": "_962w4A9wmZa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47999d5-6ab5-44e9-e6f6-071c39f55047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device is cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#    Load data and creat dictionary                           +\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "import os\n",
        "from io import open\n",
        "import torch\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids\n"
      ],
      "metadata": {
        "id": "8eIeIA0fYuVP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#    Model class                                              +\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.ntoken = ntoken\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        if rnn_type in ['LSTM', 'GRU']:\n",
        "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
        "        else:\n",
        "            try:\n",
        "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
        "            except KeyError as e:\n",
        "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
        "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\") from e\n",
        "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        # Optionally tie weights as in:\n",
        "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
        "        # https://arxiv.org/abs/1608.05859\n",
        "        # and\n",
        "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
        "        # https://arxiv.org/abs/1611.01462\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output)\n",
        "        decoded = decoded.view(-1, self.ntoken)\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "\n",
        "# Temporarily leave PositionalEncoding module here. Will be moved somewhere else.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
        "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
        "        Here, we use sine and cosine functions of different frequencies.\n",
        "    .. math:\n",
        "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "        \\text{where pos is the word position and i is the embed idx)\n",
        "    Args:\n",
        "        d_model: the embed dim (required).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        max_len: the max. length of the incoming sequence (default=5000).\n",
        "    Examples:\n",
        "        >>> pos_encoder = PositionalEncoding(d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [sequence length, batch size, embed dim]\n",
        "            output: [sequence length, batch size, embed dim]\n",
        "        Examples:\n",
        "            >>> output = pos_encoder(x)\n",
        "        \"\"\"\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "o5_NLJhdZBKQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#    Class to keep and plot results                           +\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Result_Matrix:\n",
        "    def __init__(self, model_type):\n",
        "      self.model_name = model_type\n",
        "      self.nepochs = 0\n",
        "      self.train_ppl = np.array([])\n",
        "      self.val_ppl = np.array([])\n",
        "      self.test_ppl = np.array([])\n",
        "\n",
        "    def add_result(self, result, result_type = 'train'):\n",
        "      if result_type == 'train':\n",
        "        self.train_ppl = np.append(self.train_ppl, result)\n",
        "      elif result_type == 'val':\n",
        "        self.val_ppl = np.append(self.val_ppl, result)\n",
        "      elif result_type == 'test':\n",
        "        self.test_ppl = np.append(self.test_ppl, result)\n",
        "\n",
        "    def get_results(self, result_type = 'train'):\n",
        "      if result_type == 'train':\n",
        "        return(self.train_ppl)\n",
        "      elif result_type == 'val':\n",
        "        return(self.val_ppl)\n",
        "      elif result_type == 'test':\n",
        "        return(self.test_ppl) \n",
        "\n",
        "\n",
        "def plot_results(result_matrix, file_name):\n",
        "  plt.figure()\n",
        "  y = result_matrix.get_results('train')\n",
        "  plt.plot(np.arange(1,len(y)+1), y, label = 'train', color = 'blue')\n",
        "  y = result_matrix.get_results('val')\n",
        "  plt.plot(np.arange(1,len(y)+1), y, label = 'val', color = 'green')\n",
        "  y = result_matrix.get_results('test')\n",
        "  plt.plot(np.arange(1,len(y)+1), y, label = 'test', color = 'red')\n",
        "  plt.legend()\n",
        "  plt.title(f'Model Type = {result_matrix.model_name}. Dropout = {args.dropout}')\n",
        "  #plt.ylim((0.75, 1))\n",
        "  plt.xlabel('eoch')\n",
        "  plt.ylabel('perplexity')\n",
        "  plt.grid()\n",
        "  plt.show() \n",
        "\n",
        "  plt.savefig(file_name) \n"
      ],
      "metadata": {
        "id": "fB5XPLnpFhzI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#    Classes and vunctions to prepare and run network         +\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='PyTorch Wikitext-2 RNN/LSTM/GRU Language Model')\n",
        "parser.add_argument('--data', type=str, default='./data/ptb',\n",
        "                    help='location of the data corpus')\n",
        "parser.add_argument('--model', type=str, default='LSTM',\n",
        "                    help='type of network (RNN_TANH, RNN_RELU, LSTM, GRU)')\n",
        "parser.add_argument('--emsize', type=int, default=1500,\n",
        "                    help='size of word embeddings')\n",
        "parser.add_argument('--nhid', type=int, default=200,\n",
        "                    help='number of hidden units per layer')\n",
        "parser.add_argument('--nlayers', type=int, default=2,\n",
        "                    help='number of layers')\n",
        "parser.add_argument('--lr', type=float, default=10,\n",
        "                    help='initial learning rate')\n",
        "parser.add_argument('--clip', type=float, default=0.25,\n",
        "                    help='gradient clipping')\n",
        "parser.add_argument('--epochs', type=int, default=40,\n",
        "                    help='upper epoch limit')\n",
        "parser.add_argument('--batch_size', type=int, default=20, metavar='N',\n",
        "                    help='batch size')\n",
        "parser.add_argument('--bptt', type=int, default=35,\n",
        "                    help='sequence length')\n",
        "parser.add_argument('--dropout', type=float, default=0,\n",
        "                    help='dropout applied to layers (0 = no dropout)')\n",
        "parser.add_argument('--tied', action='store_true',\n",
        "                    help='tie the word embedding and softmax weights')\n",
        "parser.add_argument('--seed', type=int, default=1111,\n",
        "                    help='random seed')\n",
        "parser.add_argument('--cuda', action='store_true', default=True,\n",
        "                    help='use CUDA')\n",
        "parser.add_argument('--mps', action='store_true', default=False,\n",
        "                        help='enables macOS GPU training')\n",
        "parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n",
        "                    help='report interval')\n",
        "parser.add_argument('--save', type=str, default='model.pt',\n",
        "                    help='path to save the final model')\n",
        "parser.add_argument('--onnx-export', type=str, default='',\n",
        "                    help='path to export the final model in onnx format')\n",
        "parser.add_argument('--nhead', type=int, default=2,\n",
        "                    help='the number of heads in the encoder/decoder of the transformer model')\n",
        "parser.add_argument('--dry-run', action='store_true',\n",
        "                    help='verify the code and the model')\n",
        "\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(args.seed)\n",
        "if torch.cuda.is_available():\n",
        "    if not args.cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda.\")\n",
        "if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    if not args.mps:\n",
        "        print(\"WARNING: You have mps device, to enable macOS GPU run with --mps.\")\n",
        "\n",
        "use_mps = args.mps and torch.backends.mps.is_available()\n",
        "if args.cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('CUDA is active')\n",
        "elif use_mps:\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "# Create Batched train, validate and test datasets\n",
        "corpus = data.Corpus(args.data)\n",
        "\n",
        "# Starting from sequential data, batchify arranges the dataset into columns.\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
        "# ┌ a g m s ┐\n",
        "# │ b h n t │\n",
        "# │ c i o u │\n",
        "# │ d j p v │\n",
        "# │ e k q w │\n",
        "# └ f l r x ┘.\n",
        "# These columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
        "# batch processing.\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, args.batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "ntokens = len(corpus.dictionary)\n",
        "\n",
        "model = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied).to(device)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "# get_batch subdivides the source data into chunks of length args.bptt.\n",
        "# If source is equal to the example output of the batchify function, with\n",
        "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
        "# ┌ a g m s ┐ ┌ b h n t ┐\n",
        "# └ b h n t ┘ └ c i o u ┘\n",
        "# Note that despite the name of the function, the subdivison of data is not\n",
        "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
        "# by the batchify function. The chunks are along dimension 0, corresponding\n",
        "# to the seq_len dimension in the LSTM.\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, args.bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data, hidden)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    \n",
        "    hidden = model.init_hidden(args.batch_size)\n",
        "       \n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        model.zero_grad()\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
        "        for p in model.parameters():\n",
        "            p.data.add_(p.grad, alpha=-lr)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % args.log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / args.log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // args.bptt, lr,\n",
        "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
        "           \n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "      \n",
        "    results_m.add_result(math.exp(cur_loss), 'train') #add train ppl to result matrix\n",
        "\n",
        "\n",
        "# Loop over epochs.\n",
        "lr = args.lr\n",
        "best_val_loss = None\n"
      ],
      "metadata": {
        "id": "p0w9cFjq_oeH",
        "outputId": "c608a2df-36d1-4d5b-fb7a-e04f113af75f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is active\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#    Main Cell to run the Network                             +\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    results_m = Result_Matrix(args.model) #create a result matrix for the model\n",
        "  \n",
        "    for epoch in range(1, args.epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        \n",
        "        \n",
        "        results_m.add_result(math.exp(val_loss), 'val') #add val ppl to result matrix\n",
        "\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(args.save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(args.save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # after load the rnn params are not a continuous chunk of memory\n",
        "    # this makes them a continuous chunk, and will speed up forward pass\n",
        "    # Currently, only rnn model supports flatten_parameters function.\n",
        "    if args.model in ['RNN_TANH', 'RNN_RELU', 'LSTM', 'GRU']:\n",
        "        model.rnn.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)\n",
        "\n",
        "results_m.add_result(math.exp(test_loss), 'test') #add test ppl to result matrix\n",
        "\n",
        "plot_results(results_m, f'{results_m.model_name} Dropout = {args.dropout != 0}.jpg')\n",
        "\n"
      ],
      "metadata": {
        "id": "PuXTYVCc3QLn",
        "outputId": "d6c7ef8a-ea1d-4bb8-8674-4160a787a833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'))\n",
            "| epoch   1 |   200/ 1327 batches | lr 10.00 | ms/batch 10.03 | loss  6.95 | ppl  1048.14\n",
            "| epoch   1 |   400/ 1327 batches | lr 10.00 | ms/batch  9.54 | loss  6.31 | ppl   548.93\n",
            "| epoch   1 |   600/ 1327 batches | lr 10.00 | ms/batch  9.57 | loss  6.01 | ppl   407.18\n",
            "| epoch   1 |   800/ 1327 batches | lr 10.00 | ms/batch  9.60 | loss  5.73 | ppl   307.42\n",
            "| epoch   1 |  1000/ 1327 batches | lr 10.00 | ms/batch  9.75 | loss  5.59 | ppl   267.04\n",
            "| epoch   1 |  1200/ 1327 batches | lr 10.00 | ms/batch  9.77 | loss  5.43 | ppl   227.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 13.32s | valid loss  5.40 | valid ppl   221.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'))\n",
            "| epoch   2 |   200/ 1327 batches | lr 10.00 | ms/batch  9.77 | loss  5.33 | ppl   206.03\n",
            "| epoch   2 |   400/ 1327 batches | lr 10.00 | ms/batch  9.72 | loss  5.26 | ppl   192.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "Exiting from training early\n",
            "=========================================================================================\n",
            "| End of training | test loss  5.38 | test ppl   216.28\n",
            "=========================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}